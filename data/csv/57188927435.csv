doi,citedby_count,description,author_names,author_ids
10.1145/3544912.3544917,2,"Software data planes running on commodity servers are very popular in real deployments. However, to attain top class performance, the software approach requires the adoption of accelerated network I/O frameworks, each of them characterized by its own programming model and API. As a result, network applications are often closely tied to the underlying technology, with obvious issues of portability over different systems. This is especially true in cloud scenarios where different I/O frameworks could be installed depending on the configuration of the physical servers in the infrastructure. The nethuns library proposes a unified programming abstraction to access and manage network operations over different I/O frameworks. The library is freely available to the community under the BSD license and currently supports AF_XDP and netmap for fast packet handling along with the classic AF_PACKET and the pcap library. Network applications based on nethuns need only to be re-compiled to run over a different network API. The experiments prove that the overhead introduced by nethuns is negligible, hence making it a convenient programming platform that eases the coding process while guaranteeing high performance and portability. As proofs of concept, a handy traffic generator as well as the popular Open vSwitch application have been successfully ported and tested over nethuns.","Bonelli, Nicola;Vigna, Fabio Del;Fais, Alessandra;Lettieri, Giuseppe;Procissi, Gregorio",55871249800;57188927435;57219562640;7003691785;6602659928
10.1038/s42005-020-0340-4,63,"Nowadays, Social Media are a privileged channel for news spreading, information exchange, and fact checking. Unexpectedly for many users, automated accounts, known as social bots, contribute more and more to this process of information diffusion. Using Twitter as a benchmark, we consider the traffic exchanged, over one month of observation, on the migration flux from Northern Africa to Italy. We measure the significant traffic of tweets only, by implementing an entropy-based null model that discounts the activity of users and the virality of tweets. Results show that social bots play a central role in the exchange of significant content. Indeed, not only the strongest hubs have a number of bots among their followers higher than expected, but furthermore a group of them, that can be assigned to the same political tendency, share a common set of bots as followers. The retweeting activity of such automated accounts amplifies the hubs’ messages.","Caldarelli, Guido;De Nicola, Rocco;Del Vigna, Fabio;Petrocchi, Marinella;Saracco, Fabio",55139905100;7004298169;57188927435;9433836000;36344077800
10.1007/s10796-018-9833-z,62,"Natural disasters, as well as human-made disasters, can have a deep impact on wide geographic areas, and emergency responders can benefit from the early estimation of emergency consequences. This work presents CrisMap, a Big Data crisis mapping system capable of quickly collecting and analyzing social media data. CrisMap extracts potential crisis-related actionable information from tweets by adopting a classification technique based on word embeddings and by exploiting a combination of readily-available semantic annotators to geoparse tweets. The enriched tweets are then visualized in customizable, Web-based dashboards, also leveraging ad-hoc quantitative visualizations like choropleth maps. The maps produced by our system help to estimate the impact of the emergency in its early phases, to identify areas that have been severely struck, and to acquire a greater situational awareness. We extensively benchmark the performance of our system on two Italian natural disasters by validating our maps against authoritative data. Finally, we perform a qualitative case-study on a recent devastating earthquake occurred in Central Italy.","Avvenuti, Marco;Cresci, Stefano;Del Vigna, Fabio;Fagni, Tiziano;Tesconi, Maurizio",6602976787;56178304900;57188927435;14017678600;55884637000
10.1109/TNSM.2018.2828939,7,"The large availability of multi-gigabit network cards for commodity PCs requires network applications to potentially cope with high volumes of traffic. However, computation intensive operations may not catch up with high traffic rates and need to be run in parallel over multiple processing cores. As of today, the vast majority of network applications - e.g., monitoring and IDS systems - are still based on the pcap library interface which, unfortunately, does not provide the native multi-core support, even though the current underlying capture technologies do. This paper introduces a novel version of the pcap library for the Linux operating system that enables transparent application level parallelism. The new library supports fan-out operations for both multi-threaded and multi-process applications, by means of extended API as well as by a declarative grammar for configuration files, suitable for legacy applications. In addition, the library can transparently run on top of the standard Linux socket as well as on other accelerated active engines. Performance evaluation has been carried out on a multi-core architecture in pure capture tests and in more realistic use cases involving monitoring applications such as Tstat and Bro, with standard Linux socket as well as PFRING and PFQ accelerated engines.","Bonelli, Nicola;Del Vigna, Fabio;Giordano, Stefano;Procissi, Gregorio",55871249800;57188927435;7101995308;6602659928
10.1016/j.osnem.2018.04.002,1,"This paper considers online news censorship and it concentrates on censorship of identities. Obfuscating identities may occur for disparate reasons, from military to judiciary ones. In the majority of cases, this happens to protect individuals from being identified and persecuted by hostile people. However, being the collaborative web characterised by a redundancy of information, it is not unusual that the same fact is reported by multiple sources, which may not apply the same restriction policies in terms of censorship. Also, the proven aptitude of social network users to disclose personal information leads to the phenomenon that comments to news can reveal the data withheld in the news itself. This gives us a mean to figure out who the subject of the censored news is. We propose an adaptation of a text analysis approach to unveil censored identities. The approach is tested on a synthesised scenario, which however resembles a real use case. Leveraging a text analysis based on a context classifier trained over snippets from posts and comments of Facebook pages, we achieve promising results. Despite the quite constrained settings in which we operate – such as considering only snippets of very short length – our system successfully detects the censored name, choosing among 10 different candidate names, in more than 50% of the investigated cases. This outperforms the results of two reference baselines. The findings reported in this paper, other than being supported by a thorough experimental methodology and interesting on their own, also pave the way for further investigation on the insidious issues of censorship on the web.","Del Vigna, Fabio;Petrocchi, Marinella;Tommasi, Alessandro;Zavattari, Cesare;Tesconi, Maurizio",57188927435;9433836000;14066911800;36086953300;55884637000
10.1007/s00146-017-0709-4,18,"Nowadays, social media analysis systems are feeding on user contributed data, either for beneficial purposes, such as emergency management, or for user profiling and mass surveillance. Here, we carry out a discussion about the power and pitfalls of public accessibility to social media-based systems, with specific regards to the emergency management application EARS (Earthquake Alert and Report System). We investigate whether opening such systems to the population at large would further strengthen the link between communities of volunteer citizens, intelligent systems, and decision makers, thus going in the direction of developing more sustainable and resilient societies. Our analysis highlights fundamental challenges and provides interesting insights into a number of research directions with the aim of developing human-centered social media-based systems.","Avvenuti, Marco;Cresci, Stefano;Vigna, Fabio Del;Tesconi, Maurizio",6602976787;56178304900;57188927435;55884637000
,211,"While favouring communications and easing information sharing, Social Network Sites are also used to launch harmful campaigns against specific groups and individuals. Cyberbullism, incitement to self-harm practices, sexual predation are just some of the severe effects of massive online offensives. Moreover, attacks can be carried out against groups of victims and can degenerate in physical violence. In this work, we aim at containing and preventing the alarming diffusion of such hate campaigns. Using Facebook as a benchmark, we consider the textual content of comments appeared on a set of public Italian pages. We first propose a variety of hate categories to distinguish the kind of hate. Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy. Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, we design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM). We test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition. The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.","Del Vigna, Fabio;Cimino, Andrea;Dell'Orletta, Felice;Petrocchi, Marinella;Tesconi, Maurizio",57188927435;57002803800;57540567000;9433836000;55884637000
10.1109/MC.2016.134,28,"To visualize post-emergency damage, a crisis-mapping system uses readily available semantic annotators, a machine-learning classifier to analyze relevant tweets, and interactive maps to rank extracted situational information. The system was validated against data from two recent disasters in Italy.","Avvenuti, Marco;Cresci, Stefano;Del Vigna, Fabio;Tesconi, Maurizio",6602976787;56178304900;57188927435;55884637000
10.1109/ICT-DM.2015.7402058,20,"Social media have become a primary communication channel among people and are continuously overwhelmed by huge volumes of User Generated Content. This is especially true in the aftermath of unpredictable disasters, when users report facts, descriptions and photos of the unfolding event. This material contains actionable information that can greatly help rescuers to achieve a better response to crises, but its volume and variety render manual processing unfeasible. This paper reports the experience we gained from developing and using a web-enabled system for the online detection and monitoring of unpredictable events such as earthquakes and floods. The system captures selected message streams from Twitter and offers decision support functionalities for acquiring situational awareness from textual content and for quantifying the impact of disasters. The software architecture of the system is described and the approaches adopted for messages filtering, emergency detection and emergency monitoring are discussed. For each module, the results of real-world experiments are reported. The modular design makes the system easy configurable and allowed us to conduct experiments on different crises, including Emilia earthquake in 2012 and Genoa flood in 2014. Finally, some possible functionalities relying on the analysis of multimedia information are introduced.","Avvenuti, Marco;Del Vigna, Fabio;Cresci, Stefano;Marchetti, Andrea;Tesconi, Maurizio",6602976787;57188927435;56178304900;23397813500;55884637000
10.1007/978-3-319-47880-7_31,4,"New Psychoactive Substances (NPS) are drugs that lay in a grey area of legislation, since they are not internationally and officially banned, possibly leading to their not prosecutable trade. The exacerbation of the phenomenon is that NPS can be easily sold and bought online. Here, we consider large corpora of textual posts, published on online forums specialized on drug discussions, plus a small set of known substances and associated effects, which we call seeds.We propose a semisupervised approach to knowledge extraction, applied to the detection of drugs (comprising NPS) and effects from the corpora under investigation. Based on the very small set of initial seeds, the work highlights how a contrastive approach and context deduction are effective in detecting substances and effects from the corpora. Our promising results, which feature a F1 score close to 0.9, pave the way for shortening the detection time of new psychoactive substances, once these are discussed and advertised on the Internet.","Del Vigna, Fabio;Petrocchi, Marinella;Tommasi, Alessandro;Zavattari, Cesare;Tesconi, Maurizio",57188927435;9433836000;14066911800;36086953300;55884637000
10.1007/978-3-319-46349-0_8,5,"Online availability and diffusion of New Psychoactive Substances (NPS) represents an emerging threat to healthcare systems. In this work, we analyse drugs forums, online shops, and Twitter. By mining the data from these sources, it is possible to understand the dynamics of drug diffusion and its endorsement, as well as timely detect new substances. We propose a set of visual analytics tools to support analysts in tackling NPS spreading and provide a better insight about drugs market and analysis.","Del Vigna, Fabio;Avvenuti, Marco;Bacciu, Clara;Deluca, Paolo;Petrocchi, Marinella;Marchetti, Andrea;Tesconi, Maurizio",57188927435;6602976787;56288087300;8676467300;9433836000;23397813500;55884637000
