doi,citedby_count,description,author_names,author_ids
10.1413/106898,0,"This paper examines some conceptual issues about the role of probability, of cost-benefit analysis, and of empirical evidence in informing public policy decisions, starting from the debate between Daniel Bernoulli and Jean Baptiste Le Rond D'Alembert on the question of whether the French government should promote a smallpox inoculation campaign. We identify Bernoulli's argument as an early example of the use of decision theory and of mathematical modeling as a tool for evaluating social prospects. Then, we explore some of D'Alembert objections to this approach, and we discuss how these criticisms survive to date both in the theoretical debate on foundational aspects of Rational Decision Theory and in the discussion over its adequacy as a model for decision-making under uncertainty.","Colombo, Camilla;Cevolani, Gustavo",57363051500;40460910700
10.1007/s10670-021-00389-7,1,"According to the so-called Lockean thesis, a rational agent believes a proposition just in case its probability is sufficiently high, i.e., greater than some suitably fixed threshold. The Preface paradox is usually taken to show that the Lockean thesis is untenable, if one also assumes that rational agents should believe the conjunction of their own beliefs: high probability and rational belief are in a sense incompatible. In this paper, we show that this is not the case in general. More precisely, we consider two methods of computing how probable must each of a series of propositions be in order to rationally believe their conjunction under the Lockean thesis. The price one has to pay for the proposed solutions to the paradox is what we call “quasi-dogmatism”: the view that a rational agent should believe only those propositions which are “nearly certain” in a suitably defined sense.","Bonzio, Stefano;Cevolani, Gustavo;Flaminio, Tommaso",56955630000;40460910700;8886511600
10.1007/978-3-031-10135-9_71,0,"The chapter presents the problem of reverse inference in cognitive neuroscience, discussing its analysis in terms of abductive and Bayesian reasoning and highlighting the main methodological issues and open problems surrounding this crucial inferential practice with reference to case studies from current research.","Coraci, Davide;Calzavarini, Fabrizio;Cevolani, Gustavo",57235699600;57193349547;40460910700
10.1007/978-3-031-10135-9_70,0,"Cognitive neuroscience is the study of the biological, and especially neural, processes that underlie cognition and mental activities. The three chapters in this Section of the Handbook explore abductive reasoning with a special focus on inference patterns routinely employed by neuroscientists both in evaluating evidence concerning brain activity and in assessing cognitive hypotheses on its basis.","Cevolani, Gustavo",40460910700
10.1422/106994,0,"In many fields, experts must reason and make decisions under conditions of uncertainty more or less severe. This is especially true within the courts: understanding how uncertainty affects the processes of reasoning, inference, and decision-making in the judiciary is a crucial and urgent issue. In this paper, we analyze judicial reasoning in the context of evidence evaluation in the criminal trial, with reference to the debate on “legal probabilism”, i.e., the idea that judicial reasoning should be analyzed with the tools of probability and decision theory. Against legal probabilism, a number of objections have been raised, including the so-called puzzles of (merely) statistical evidence, which seem to show the untenability of that idea, especially in relation to the principle of “beyond a reasonable doubt” as a standard of evidence. Using cognitive decision theory as an epistemological framework, we defend a qualified version of legal probabilism, based on the notion of truthlikeness (or verisimilitude or approximation to truth) as it is studied in contemporary philosophy of science.","Cevolani, Gustavo;Pirisi, Matteo",40460910700;58290496800
10.1007/978-3-031-28390-1_11,0,"In the past decades, research in cognitive science has increasingly improved our understanding of human cognition and decision-making processes. On the one hand, the “heuristics and biases” research program highlighted how human reasoners often fail to conform to normative standards of rationality, falling prey to systematic and predictable “cognitive illusions”. On the other hand, further research explored how to design communication strategies and reasoning tools that are more ergonomic - that is, that make easier and more transparent the processing and evaluation of complex information. Here, we report on a preliminary analysis (including a pilot study) of understanding and communication of statistical information concerning COVID-19-related risks among the public. Our tentative conclusions are as follows: first, in line with previous literature, transparent communication using a “natural frequency” format (instead of percentages or probabilities) does improve understanding; second, many important institutions do not employ cognitively ergonomic communication strategies, leaving much room for further improvement.","Coraci, Davide;Demichelis, Alessandro;Cevolani, Gustavo",57235699600;58164531000;40460910700
10.1007/s11229-022-04022-0,0,"This paper contributes to the debate on the question of whether a systematic connection obtains between one’s commitment to realism or antirealism and one’s attitude towards the possibility of radical theoretical novelty, namely, theory change affecting our best, most successful theories (see, e.g., Stanford in Synthese 196:3915–3932, 2019; Dellsén in Stud Hist Philos Sci 76:30–38, 2019). We argue that it is not allegiance to realism or antirealism as such that primarily dictates one’s response to the possibility of radical theoretical novelty: what matters the most is, rather, the proposed alternative’s promise to realize one’s favored cognitive aim(s). Our argument tells not only against Stanford’s account of how adherence to realism or antirealism orients how one responds to possible radical theoretical novelty, but also against what we call the “natural pairing thesis.” According to such a thesis, which has kept resurfacing in the history of the philosophy of science, one-to-one pairings obtain between realism/antirealism, on the one hand, and theoretical conservatism/openness to radical theoretical novelty, on the other hand. As our argument suggests, however, when faced with the possibility of radical theoretical novelty, realists can respond either in a conservative way or by being open to theory change, not unlike antirealists.","Tambolo, Luca;Cevolani, Gustavo",54966821700;40460910700
10.1007/s11229-022-03516-1,1,"After Karl Popper’s original work, several approaches were developed to provide a sound explication of the notion of verisimilitude. With few exceptions, these contributions have assumed that the truth to be approximated is deterministic. This collection of ten papers addresses the more general problem of approaching probabilistic truths. They include attempts to find appropriate measures for the closeness to probabilistic truth and to evaluate claims about such distances on the basis of empirical evidence. The papers employ multiple analytical approaches, and connect the research to related issues in the philosophy of science.","Niiniluoto, Ilkka;Cevolani, Gustavo;Kuipers, Theo",6602584996;40460910700;16413282100
10.1007/s11229-022-03585-2,8,"Reverse inference is a crucial inferential strategy used in cognitive neuroscience to derive conclusions about the engagement of cognitive processes from patterns of brain activation. While widely employed in experimental studies, it is now viewed with increasing scepticism within the neuroscience community. One problem with reverse inference is that it is logically invalid, being an instance of abduction in Peirce’s sense. In this paper, we offer the first systematic analysis of reverse inference as a form of abductive reasoning and highlight some relevant implications for the current debate. We start by formalising an important distinction that has been entirely neglected in the literature, namely the distinction between weak (strategic) and strong (justificatory) reverse inference. Then, we rely on case studies from recent neuroscientific research to systematically discuss the role and limits of both strong and weak reverse inference; in particular, we offer the first exploration of weak reverse inference as a discovery strategy within cognitive neuroscience.","Calzavarini, Fabrizio;Cevolani, Gustavo",57193349547;40460910700
10.1007/s11023-022-09592-z,14,"The idea that “simplicity is a sign of truth”, and the related “Occam’s razor” principle, stating that, all other things being equal, simpler models should be preferred to more complex ones, have been long discussed in philosophy and science. We explore these ideas in the context of supervised machine learning, namely the branch of artificial intelligence that studies algorithms which balance simplicity and accuracy in order to effectively learn about the features of the underlying domain. Focusing on statistical learning theory, we show that situations exist for which a preference for simpler models (as modeled through the addition of a regularization term in the learning problem) provably slows down, instead of favoring, the supervised learning process. Our results shed new light on the relations between simplicity and truth approximation, which are briefly discussed in the context of both machine learning and the philosophy of science.","Bargagli Stoffi, Falco J.;Cevolani, Gustavo;Gnecco, Giorgio",57207772321;40460910700;22953696000
10.1007/978-3-031-11744-2_6,1,"In recent decades, empirical investigation has increasingly illuminated how experts in the legal domain, including judges, evaluate evidence and hypotheses, reason and decide about them. Research has highlighted both the cognitive strategies employed in legal reasoning, and the cognitive pitfalls judges and other experts tend to fall prey to. In this paper, we focus on the “conjunction fallacy”, a widespread phenomenon showing that human reasoners systematically violate the rules of probability calculus. After presenting the fallacy as documented in judicial reasoning, we present two formal accounts of the phenomenon, respectively based on the notions of confirmation (evidential support) and truthlikeness (closeness to the truth) as studied in the philosophy of science. With reference to the “story-model” of legal decision-making, we clarify the role that “cognitive utilities” like truth, probability, and information play in legal reasoning, and how it can account for the documented fallacies. We conclude by suggesting some directions for further investigation.","Cevolani, Gustavo;Crupi, Vincenzo",40460910700;7005909406
10.1422/105037,0,"“Reverse” inference is a key reasoning strategy used by neuroscientists to derive conclusions about the engagement of cognitive processes from patterns of brain activation observed in fMRI experiments. Reverse inference is usually opposed to “forward” inference, when the engagement of a certain cognitive process is assumed, and its neural correlate is inferred from the experimental evidence. Despite its central role in neuroscientific practice, in recent years reverse inference faced increasing skepticism, especially after leading neuroscientist Russell Poldrack denounced its limits and espoused its methodological weaknesses. In this paper, we critically discuss Poldrack’s Bayesian analysis of reverse inference, and point to some pros and cons of his proposal. The upshot of our discussion is that, while the Bayesian account of reverse inference is ultimately sound and promising, its current application in the neuroscience literature faces some important issues. In particular, we argue that the widespread use of flat priors in analyzing data i) blurs a crucial distinction between probability and confirmation, ii) makes the Bayesian analysis collapsing on a “likelihoodist” one, and iii) risks to conflate the basic concepts of forward and reverse inference.","Coraci, Davide;Cevolani, Gustavo",57235699600;40460910700
10.1177/00483931211049759,1,"This paper defends the viability of de-idealization strategies in economic modeling against recent criticism. De-idealization occurs when an idealized assumption of a theoretical model is replaced with a more realistic one. Recently, some scholars have raised objections against the possibility or fruitfulness of de-idealizing economic models, suggesting that economists do not employ this kind of strategy. We present a detailed case study from the theory of industrial organization, discussing three different models, two of which can be construed as de-idealized versions of the first (the so-called Bertrand model of oligopoly). We conclude that recent pessimism about de-idealization in economics is largely unfounded, and that de-idealization strategies are not only possible but also widely employed in economics.","Peruzzi, Edoardo;Cevolani, Gustavo",57360431000;40460910700
10.1016/j.shpsa.2021.09.001,3,"When two or more (groups of) researchers independently investigating the same domain arrive at the same result, a multiple discovery occurs. The pervasiveness of multiple discoveries in science suggests the intuition that they are in some sense inevitable—that one should view them as results that force themselves upon us, so to speak. We argue that, despite the intuitive force of such an “inevitabilist insight,” one should reject it. More specifically, we distinguish two facets of the insight and argue that: (a) the profusion of multiple discoveries in scientific practice does not support the inevitabilist side of the inevitability/contingency of science controversy; and (b) the crucial role of background knowledge in scientific inquiry complicates the attempt to interpret the pervasiveness of multiple discoveries in realist terms.","Tambolo, Luca;Cevolani, Gustavo",54966821700;40460910700
10.1007/s11229-021-03298-y,1,"The basic problem of a theory of truth approximation is defining when a theory is “close to the truth” about some relevant domain. Existing accounts of truthlikeness or verisimilitude address this problem, but are usually limited to the problem of approaching a “deterministic” truth by means of deterministic theories. A general theory of truth approximation, however, should arguably cover also cases where either the relevant theories, or “the truth”, or both, are “probabilistic” in nature. As a step forward in this direction, we first present a general characterization of both deterministic and probabilistic truth approximation; then, we introduce a new account of verisimilitude which provides a simple formal framework to deal with such issue in a unified way. The connections of our account with some other proposals in the literature are also briefly discussed.","Cevolani, Gustavo;Festa, Roberto",40460910700;36745426900
10.1007/s10670-018-0087-4,1,"Knowledge representation is a central issue in a number of areas, but few attempts are usually made to bridge different approaches accross different fields. As a contribution in this direction, in this paper I focus on one such approach, the theory of conceptual spaces developed within cognitive science, and explore its potential applications in the fields of philosophy of science and formal epistemology. My case-study is provided by the theory of truthlikeness (or verisimilitude), construed as closeness to “the whole truth” about a given domain, as described in the underlying language. I show how modeling propositions and their relations within a conceptual space has interesting implications for two issues in truthlikeness theory: the so called problem of language dependence, and that of measure sensitivity. I conclude by pointing at some open issues arising from the application of conceptual spaces to the analysis of philosophical problems.","Cevolani, Gustavo",40460910700
10.1007/s11229-018-01947-3,12,"Popper’s original definition of truthlikeness relied on a central insight: that truthlikeness combines truth and information, in the sense that a proposition is closer to the truth the more true consequences and the less false consequences it entails. As intuitively compelling as this definition may be, it is untenable, as proved long ago; still, one can arguably rely on Popper’s intuition to provide an adequate account of truthlikeness. To this aim, we mobilize some classical work on partial entailment in defining a new measure of truthlikeness which satisfies a number of desiderata. The resulting account has some interesting and surprising connections with other accounts on the market, thus shedding new light on current attempts of systematizing different approaches to verisimilitude.","Cevolani, Gustavo;Festa, Roberto",40460910700;36745426900
10.1007/978-3-030-29179-2_9,0,"The use of a certain drug or treatment in the cure of a disease or condition should be based on appropriate tests of the hypothesis that said drug or treatment is efficacious in the cure of the disease or condition at hand. In this paper, we aim at elucidating how such “efficacy hypotheses” are tested and evaluated, especially in clinical trials. More precisely, we shall suggest that the principles governing the assessment of efficacy hypotheses, and, more generally, hypotheses of statistical causality, are provided by an appropriate statistical version of such a venerable procedure as the method of difference put forward by John Stuart Mill.","Festa, Roberto;Cevolani, Gustavo;Tambolo, Luca",36745426900;40460910700;54966821700
10.4453/rifp.2020.0022,0,"The global emergency caused by the spread of COVID-19 raises critical challenges for individuals and communities on many different levels. In particular, politicians, scientists, physicians, and other professionals may face new ethical dilemmas and cognitive constraints as they make critical decisions in extraordinary circumstances. Philosophers and cognitive scientists have long analyzed and discussed such issues. An example is the debate on moral decision making in imaginary scenarios, such as the famous “Trolley Problem”. Similarly, dramatic and consequential decisions are realized daily in the current crisis. Focusing on Italy, we discuss the clinical ethical guidelines proposed by the Italian Society of Anesthesiology, Analgesia, Resuscitation and Intensive Care (SIAARTI), highlighting some crucial ethical and cognitive concerns surrounding emergency decision making in the current situation.","Lucifora, Chiara;Cevolani, Gustavo",57209361987;40460910700
10.1007/978-3-030-64583-0_6,2,"In this short paper, a theoretical analysis of Occam’s razor formulation through statistical learning theory is presented, showing that pathological situations exist for which regularization may slow down supervised learning instead of making it faster.","Bargagli-Stoffi, Falco;Cevolani, Gustavo;Gnecco, Giorgio",57207772321;40460910700;22953696000
10.1111/cogs.12613,50,"Searching for information is critical in many situations. In medicine, for instance, careful choice of a diagnostic test can help narrow down the range of plausible diseases that the patient might have. In a probabilistic framework, test selection is often modeled by assuming that people's goal is to reduce uncertainty about possible states of the world. In cognitive science, psychology, and medical decision making, Shannon entropy is the most prominent and most widely used model to formalize probabilistic uncertainty and the reduction thereof. However, a variety of alternative entropy metrics (Hartley, Quadratic, Tsallis, Rényi, and more) are popular in the social and the natural sciences, computer science, and philosophy of science. Particular entropy measures have been predominant in particular research areas, and it is often an open issue whether these divergences emerge from different theoretical and practical goals or are merely due to historical accident. Cutting across disciplinary boundaries, we show that several entropy and entropy reduction measures arise as special cases in a unified formalism, the Sharma–Mittal framework. Using mathematical results, computer simulations, and analyses of published behavioral data, we discuss four key questions: How do various entropy models relate to each other? What insights can be obtained by considering diverse entropy models within a unified framework? What is the psychological plausibility of different entropy models? What new questions and insights for research on human information acquisition follow? Our work provides several new pathways for theoretical and empirical research, reconciling apparently conflicting approaches and empirical findings within a comprehensive and unified information-theoretic formalism.","Crupi, Vincenzo;Nelson, Jonathan D.;Meder, Björn;Cevolani, Gustavo;Tentori, Katya",7005909406;55169673400;24765094800;40460910700;6505965129
10.1080/00048402.2016.1224265,14,"The so-called Preface Paradox seems to show that one can rationally believe two logically incompatible propositions. We address this puzzle, relying on the notions of truthlikeness and approximate truth as studied within the post-Popperian research programme on verisimilitude. In particular, we show that adequately combining probability, approximate truth, and truthlikeness leads to an explanation of how rational belief is possible in the face of the Preface Paradox. We argue that our account is superior to other solutions of the paradox, including a recent one advanced by Hannes Leitgeb (Analysis 74.1).","Cevolani, Gustavo;Schurz, Gerhard",40460910700;6602897379
10.1007/s10670-016-9811-0,11,"The Preface Paradox apparently shows that it is sometimes rational to believe logically incompatible propositions. In this paper, I propose a way out of the paradox based on the ideas of fallibilism and verisimilitude (or truthlikeness). More precisely, I defend the view that a rational inquirer can fallibly believe or accept a proposition which is false, or likely false, but verisimilar; and I argue that this view makes the Preface Paradox disappear. Some possible objections to my proposal, and an alternative view of fallible belief, are briefly discussed in the final part of the paper.","Cevolani, Gustavo",40460910700
,0,"Notions of entropy and uncertainty are fundamental to many domains, ranging from the philosophy of science to physics. One important application is to quantify the expected usefulness of possible experiments (or questions or tests). Many different entropy models could be used; different models do not in general lead to the same conclusions about which tests (or experiments) are most valuable. It is often unclear whether this is due to different theoretical and practical goals or are merely due to historical accident. We introduce a unified two-parameter family of entropy models that incorporates a great deal of entropies as special cases. This family of models offers insight into heretofore perplexing psychological results, and generates predictions for future research.","Nelson, Jonathan D.;Crupi, Vincenzo;Meder, Björn;Cevolani, Gustavo;Tentori, Katya",55169673400;7005909406;24765094800;40460910700;6505965129
10.1007/978-3-319-53730-6_21,5,"The so-called problem of measure sensitivity concerns the use of formal models in philosophical argumentation: as it turns out, the soundness of many arguments is critically sensitive to the choice of the specific models employed. In this paper, I study how this issue affects the theory of truthlikeness (or verisimilitude) and its applications. As an illustration, I focus on the idea of cognitive progress as increasing truthlikeness. In particular, I show that some basic arguments concerning truth approximation through belief change are not invariant across the different truthlikeness measures proposed in the literature.","Cevolani, Gustavo",40460910700
10.1086/688935,8,"We explore the grammar of Bayesian confirmation by focusing on some likelihood principles, including theWeak Law of Likelihood.We show that none of the likelihood principles proposed so far is satisfied by all incremental measures of confirmation, and we argue that some of these measures indeed obey new, prima facie strange, antilikelihood principles. To prove this, we introduce a new measure that violates the Weak Law of Likelihood while satisfying a strong antilikelihood condition. We conclude by hinting at some relevant links between the likelihood principles considered here and other properties of Bayesian confirmation recently explored in the literature.","Festa, Roberto;Cevolani, Gustavo",36745426900;40460910700
10.1093/jigpal/jzw021,4,"Theories of truthlikeness (or verisimilitude) are currently being classified according to two independent distinctions: thatbetween 'content' and 'likeness' accounts, and that between 'conjunctive' and 'disjunctive' ones. In this article, I presentand discuss a new definition of truthlikeness, which employs Carnap's notion of the content elements entailed by a theoryor proposition, and is then labelled 'Carnapian'. After studying in detail the properties and shortcomings of this definition, Iargue that it occupies a unique position in the landscape of different approaches to truthlikeness. More precisely, I show thatit provides the only explication of truthlikeness which is both 'conjunctive' and 'content-based' in a suitably defined sense.","Cevolani, Gustavo",40460910700
10.1007/978-3-319-23015-3_5,0,,"Cevolani, Gustavo",40460910700
10.1007/978-3-319-23015-3_5,0,"The problem of the truth conduciveness of different epistemic practices is a central issue in so called veristic social epistemology. I address this issue by focusing on the “theory of dialectical structures” developed by Gregor Betz (Debate Dynamics: How Controversy Improves Our Beliefs, Springer, 2013). I link this theory to accounts of truth approximation in the philosophy of science, and explore their formal and conceptual relationships. I then argue that the notion of verisimilitude or truthlikeness could be usefully applied within the theory of dialectical structures to provide a richer account of how debate dynamics can be truth conducive. This notion also allows for a distinction between the problem of truth approximation and the problem of truth tracking, which is briefly discussed in the last section.","Cevolani, Gustavo",40460910700
10.1007/s11229-014-0486-2,9,"In this paper, we investigate the problem of truth approximation via belief merging, i.e., we ask whether, and under what conditions, a group of inquirers merging together their beliefs makes progress toward the truth about the underlying domain. We answer this question by proving some formal results on how belief merging operators perform with respect to the task of truth approximation, construed as increasing verisimilitude or truthlikeness. Our results shed new light on the issue of how rational (dis)agreement affects the inquirers’ quest for truth. In particular, they vindicate the intuition that scientific inquiry, and rational discussion in general, benefits from some heterogeneity in opinion and interaction among different viewpoints. The links between our approach and related analyses of truth tracking, judgment aggregation, and opinion dynamics, are also highlighted.","Cevolani, Gustavo",40460910700
10.1007/978-3-319-06080-4_5,5,"Some authors, most notably Luciano Floridi, have recently argued for a notion of “strongly” semantic information, according to which information “encapsulates” truth (the so-called “veridicality thesis”). We propose a simple framework to compare different formal explications of this concept and assess their relative merits. It turns out that the most adequate proposal is that based on the notion of “partial truth”, which measures the amount of “information about the truth” conveyed by a given statement. We conclude with some critical remarks concerning the veridicality thesis in connection with the role played by truth and information as relevant cognitive goals of inquiry.","Cevolani, Gustavo",40460910700
,1,"The provision of public goods is an important problem in economics and the social sciences. It is often claimed that this problem has the structure of the well-known Prisoner&#39;s Dilemma so that rational and self-interested individuals would not be able to provide any public good by spontaneous cooperation. In this paper, we argue that this pessimistic view of the possibility spontaneous cooperation is largely unjustified, since the game theoretic analysis of public goods shows how their voluntary provision is indeed feasible in a number of circumstances. We conclude by considering the implications of the game theoretic approach to the problem of public goods for political philosophy and, in particular, for the possibility of &quot;ordered anarchy&quot; as defended in the works of Anthony de Jasay.","Cevolani, Gustavo;Festa, Roberto",40460910700;36745426900
,0,,"Cevolani, Gustavo",40460910700
10.1016/j.shpsa.2013.05.004,4,"In a recent paper entitled ""Truth does not explain predictive success"" (Analysis, 2011), Carsten Held argues that the so-called ""No-Miracles Argument"" for scientific realism is easily refuted when the consequences of the underdetermination of theories by the evidence are taken into account. We contend that the No-Miracles Argument, when it is deployed within the context of sophisticated versions of realism, based on the notion of truthlikeness (or verisimilitude), survives Held's criticism unscathed. © 2013 Elsevier Ltd.","Cevolani, Gustavo;Tambolo, Luca",40460910700;54966821700
10.1007/s11229-012-0165-0,19,"In this paper, we address the problem of truth approximation through theory change, asking whether revising our theories by newly acquired data leads us closer to the truth about a given domain. More particularly, we focus on ""nomic conjunctive theories"", i.e., theories expressed as conjunctions of logically independent statements concerning the physical or, more generally, nomic possibilities and impossibilities of the domain under inquiry. We define both a comparative and a quantitative notion of the verisimilitude of such theories, and identify suitable conditions concerning the (partial) correctness of acquired data, under which revising our theories by data leads us closer to ""the nomic truth"", construed as the target of scientific inquiry. We conclude by indicating some further developments, generalizations, and open issues arising from our results. © 2012 Springer Science+Business Media B.V.","Cevolani, Gustavo;Festa, Roberto;Kuipers, Theo A.F.",40460910700;36745426900;16413282100
10.1007/s10670-012-9362-y,47,"In this paper we provide a compact presentation of the verisimilitudinarian approach to scientific progress (VS, for short) and defend it against the sustained attack recently mounted by Alexander Bird (2007). Advocated by such authors as Ilkka Niiniluoto and Theo Kuipers, VS is the view that progress can be explained in terms of the increasing verisimilitude (or, equivalently, truthlikeness, or approximation to the truth) of scientific theories. According to Bird, VS overlooks the central issue of the appropriate grounding of scientific beliefs in the evidence, and it is therefore unable (a) to reconstruct in a satisfactory way some hypothetical cases of scientific progress, and (b) to provide an explanation of the aversion to falsity that characterizes scientific practice. We rebut both of these criticisms and argue that they reveal a misunderstanding of some key concepts underlying VS. © 2012 Springer Science+Business Media B.V.","Cevolani, Gustavo;Tambolo, Luca",40460910700;54966821700
10.1007/s10670-011-9290-2,45,"Theory change is a central concern in contemporary epistemology and philosophy of science. In this paper, we investigate the relationships between two ongoing research programs providing formal treatments of theory change: the (post-Popperian) approach to verisimilitude and the AGM theory of belief change. We show that appropriately construed accounts emerging from those two lines of epistemological research do yield convergences relative to a specified kind of theories, here labeled ""conjunctive"". In this domain, a set of plausible conditions are identified which demonstrably capture the verisimilitudinarian effectiveness of AGM belief change, i. e., its effectiveness in tracking truth approximation. We conclude by indicating some further developments and open issues arising from our results. © 2011 Springer Science+Business Media B.V.","Cevolani, Gustavo;Crupi, Vincenzo;Festa, Roberto",40460910700;7005909406;36745426900
10.1007/978-90-481-3263-8_5,9,"Starting from the 1960s of the past century theory change has become a main concern of philosophy of science. Two of the best known formal accounts of theory change are the post-Popperian theories of verisimilitude (PPV for short) and the AGM theory of belief change (AGM for short). In this paper, we will investigate the conceptual relations between PPV and AGM and, in particular, we will ask whether the AGM rules for theory change are effective means for approaching the truth, i.e., for achieving the cognitive aim of science pointed out by PPV. First, the key ideas of PPV and AGM and their application to a particular kind of propositional theories-the so called conjunctive propositions-will be illustrated. Afterwards, we will prove that, as far as conjunctive propositions are concerned, AGM belief change is an effective tool for approaching the truth. © 2010 Springer Science+Business Media B.V.","Cevolani, Gustavo;Calandra, Francesco",40460910700;56000137000
