import os
import pickle

import sqlite3
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity


def calculate_cosine_distance(dataframe, column_name='digital_image', which_method='average'):
    # Initialize a list to store the cosine distances
    distances = []

    # Compare each document with every other document
    for i in range(len(dataframe[column_name])):
        for j in range(i + 1, len(dataframe[column_name])):
            similarity_score = cosine_similarity([dataframe.at[i, column_name]], [dataframe.at[j, column_name]])
            cosine_distance = 1 - similarity_score[0][0]  # Calculate cosine distance
            distances.append(cosine_distance)

    # Compute either the average or the standard deviation of the distances
    if which_method == 'average':
        return np.mean(distances)
    elif which_method == 'std':
        return np.std(distances)
    elif which_method == 'both':
        return np.std(distances), np.mean(distances)
    else:
        raise ValueError("Invalid method. Choose 'average' or 'std'.")


### sarebbe interessante vedere all'interno della stessa area tematica: esempio blindness, qual è
## la distanza media tra i documenti che hanno come area tematica blindness, così non si sovrappone
## con la questione dell'interdisciplinarità

def h_index(column_citations):
    citations = sorted(column_citations, reverse=True)
    h_index = 0
    for i in range(len(citations)):
        if citations[i] >= i + 1:
            h_index = i + 1
    return h_index


def combined_indices(dataframe):
    # Calculate the h-index
    dataframe['h_index'] = h_index(dataframe['citedby_count'])

    # Calculate the average cosine distance
    dataframe['average_cosine_distance'] = calculate_cosine_distance(dataframe)

    dataframe_final = dataframe[['h_index', 'average_cosine_distance']]

    return dataframe_final


if __name__ == '__main__':
    # # Connect to the database
    db_path = os.path.join(os.path.dirname(__file__),
                           '..', 'data', 'db', 'e2index.db')

    conn = sqlite3.connect(db_path)

    # STEP 1: For each author in the DB, load the papers list
    df_author_paper = pd.read_sql('SELECT * FROM AuthorPaper', conn)
    papers_by_author = df_author_paper.groupby('author_id')['doi'].apply(list)
    for author, papers in papers_by_author.items():
        papers_set = set(papers)
        formatted_paper_ids = ', '.join(f"'{doi}'" for doi in papers_set)
        query = f"SELECT * FROM Paper WHERE doi IN ({formatted_paper_ids})"

        # STEP 2: For each author, load the digital image of each paper
        df_paper = pd.read_sql(query, conn)
        df_paper['digital_image'] = df_paper['digital_image_pickled'].apply(pickle.loads)
        df_paper = df_paper.drop('digital_image_pickled', axis=1)

        # STEP 3: For each author, load calculate the similarity matrix
        print(f'Calculating indices for author {author}')
        df_result = combined_indices(df_paper)

        # STEP 4: Save results to the DB
        df_result['author_id'] = author
        df_result.set_index('author_id', inplace=True)
        df_result.to_sql('Index', conn, if_exists='append', index=True, index_label='author_id')

    # Close the connection
    conn.close()
