import os
import pickle

import sqlite3
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity


def calculate_cosine_distance(dataframe, column_name='digital_image', which_method='average'):
    # # Initialize a list to store the cosine distances
    # distances = []
    #
    # Compare each document with every other document
    # for i in range(len(dataframe[column_name])):
    #     for j in range(i + 1, len(dataframe[column_name])):
    #         similarity_score = cosine_similarity([dataframe.at[i, column_name]], [dataframe.at[j, column_name]])
    #         cosine_distance = 1 - similarity_score[0][0]  # Calculate cosine distance
    #         distances.append(cosine_distance)

    # Convert the column of vectors to a NumPy array
    vectors = np.stack(dataframe[column_name].values)

    # Calculate cosine distance
    cosine_distance_matrix = 1 - cosine_similarity(vectors)

    # Extract the upper triangular part of the distance matrix, excluding the diagonal
    # This avoids redundant calculations for pairs and self-comparison
    i_upper = np.triu_indices_from(cosine_distance_matrix, k=1)
    distances = cosine_distance_matrix[i_upper]

    # Compute either the average or the standard deviation of the distances
    if which_method == 'average':
        return np.mean(distances)
    elif which_method == 'std':
        return np.std(distances)
    elif which_method == 'both':
        return np.std(distances), np.mean(distances)
    else:
        raise ValueError("Invalid method. Choose 'average' or 'std'.")

# Function proposed by Emanuele Agrimi - average abstract and we calculate the distance of each abstract from the average abstract 
def calculate_distance_from_average_abstract(dataframe, column_name='digital_image', which_method='average'):
    # caculate the average abstract
    average_abstract = np.mean(dataframe[column_name].values, axis=0)
    distance_from_average_abstract = [1 - cosine_similarity([average_abstract], [abstract])[0] for abstract in dataframe[column_name].values]
    return np.mean(distance_from_average_abstract)


def h_index(column_citations):
    citations = sorted(column_citations, reverse=True)
    h_index = 0
    for i in range(len(citations)):
        if citations[i] >= i + 1:
            h_index = i + 1
    return h_index


def combined_indices(dataframe):
    # Calculate the h-index
    h_ind = h_index(dataframe['citedby_count'])

    # Calculate the average cosine distance
    # TODO optimize this!
    e2_ind = calculate_cosine_distance(dataframe)

    return h_ind, e2_ind


def clean_up_db_table(conn, table_name):
    cur = conn.cursor()

    # Create a new table with the same structure
    cur.execute(f"CREATE TABLE {table_name}_new AS SELECT DISTINCT * FROM {table_name}")

    # Delete the old table
    cur.execute(f"DROP TABLE {table_name}")

    # Rename the new table to the old table's name
    cur.execute(f"ALTER TABLE {table_name}_new RENAME TO {table_name}")

    # Commit the changes and close the cursor
    conn.commit()
    cur.close()


if __name__ == '__main__':
    # # Connect to the database
    db_path = os.path.join(os.path.dirname(__file__),
                           '..', 'data', 'db', 'e2index.db')

    conn = sqlite3.connect(db_path)

    # STEP 1: For each author in the DB, load the papers list
    df_author_paper = pd.read_sql('SELECT * FROM AuthorPaper', conn)
    papers_by_author = df_author_paper.groupby('author_id')['doi'].apply(list)
    for author, papers in papers_by_author.items():
        papers_set = set(papers)
        formatted_paper_ids = ', '.join(f"'{doi}'" for doi in papers_set)
        query = f"SELECT * FROM Paper WHERE doi IN ({formatted_paper_ids})"

        # STEP 2: For each author, load the digital image of each paper
        df_paper = pd.read_sql(query, conn)
        df_paper['digital_image'] = df_paper['digital_image_pickled'].apply(pickle.loads)
        df_paper = df_paper.drop('digital_image_pickled', axis=1)

        # STEP 3: For each author, load calculate the similarity matrix
        print(f'Calculating indices for author {author}')
        h_ind, e2_ind = combined_indices(df_paper)

        # STEP 4: Save results to the DB
        # TODO enrich results with author_name and unit
        unit_path = os.path.join(os.path.dirname(__file__),
                                 '..', 'data', 'csv', 'imt', 'professors.csv')
        df_unit = pd.read_csv(unit_path)
        df_unit.set_index('ID', inplace=True)

        df_result = pd.DataFrame.from_dict({'author_id': [author], 'h_index': [h_ind], 'e2_index': [e2_ind]})
        df_result.set_index('author_id', inplace=True)

        df_result = df_result.merge(df_unit, left_index=True, right_index=True, how='left')

        df_result.to_sql('E2Index', conn, if_exists='append', index=True, index_label='author_id')

    # Remove duplicates from Index DB
    print('Cleaning up the DB')
    clean_up_db_table(conn, 'E2Index')

    # Close the connection
    conn.close()
