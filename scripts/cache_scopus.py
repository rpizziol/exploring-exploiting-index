import os
import sqlite3
import json
import pickle

import numpy as np
import pandas as pd
import spacy

import requests
from pybliometrics.scopus import ScopusSearch


def cache_scopus(overwrite=False):
    # read the authors list
    authors_path = os.path.join(os.path.dirname(__file__),
                                '..', 'data', 'txt', 'authors.txt')

    with open(authors_path, 'r') as file:
        authors = file.readlines()

    print(f'Found {len(authors)} authors.')

    # Prepare the database
    # Connect to SQLite database (will be created if not exists)
    db_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'db', 'e2index.db')
    conn = sqlite3.connect(db_path)

    # Prepare the NLP model
    nlp_fn = spacy.load('en_core_web_md')

    # for each author, get the papers from scopus
    for author in authors:
        # "clean" author
        # TODO make safer conversion
        author = int(author.strip())

        # check that author is in the DB
        if author_exists_db(author, conn) and not overwrite:
            continue

        # get the papers via SCOPUS Web API
        papers_df = get_papers_easy(author)

        # TODO REMOVE: interim write and read json to avoid API calls
        # paper_csv_path = os.path.join(os.path.dirname(__file__),
        #                               '..', 'data', 'csv', f'{author}.csv')
        # papers_df.to_csv(paper_csv_path, index=False)

        # papers_df = pd.read_csv(paper_csv_path)

        # save the papers into the DB
        save_to_db(author, papers_df, conn, nlp_fn)

    # TODO Remove duplicates from the DB
    clean_up_db(conn)

    # Bookkeeping
    conn.close()


def get_papers_by_id(api_key, author_id):
    papers_raw = None

    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "X-ELS-APIKey": api_key,
        "Accept": "application/json"
    }
    params = {
        "query": f"AU-ID({author_id})",
        "field": "doi,citedby-count"
    }

    response = requests.get(base_url, headers=headers, params=params)

    if response.status_code == 200:
        papers_raw = response.json()
    else:
        print("Failed to retrieve data:", response.status_code)

    return papers_raw


def process_web_data(papers_raw):
    # Navigate through JSON structure to find the papers data
    papers_list = papers_raw.get('search-results', {}).get('entry', [])

    # Extract relevant data for each paper
    processed_data = []
    for paper in papers_list:
        paper_data = {
            'DOI': paper.get('prism:doi', None),
            'citedby-count': paper.get('citedby-count', None)
        }
        processed_data.append(paper_data)

    # Create DataFrame
    papers_df = pd.DataFrame(processed_data)
    return papers_df


def get_papers_easy(author_id):
    print(f'Getting papers for author: {author_id}')

    # request to the SCOPUS API
    q = f"AU-ID({author_id})"  # any query that works in the Advanced Search on scopus.com
    s = ScopusSearch(q)

    # convert to DataFrame
    df = pd.DataFrame(s.results)

    # extract useful columns
    selected_cols = ['doi', 'citedby_count', 'description', 'author_names', 'author_ids']
    df = df[selected_cols]

    return df


def save_to_db(author, df, conn, nlp_fn):
    print(f'Saving papers to the DB for author: {author}')
    # Write 2 DataFrames to SQL tables
    # Paper - doi, citedby_count, *digital_image [doi is a unique key]
    # AuthorPaper - author_id, doi [many-to-many]
    # (Optional) Author - author_id, author_name, *digital_image [author_id is a unique key]
    # prepare the data

    # Prepare
    # selected_cols = ['doi', 'citedby_count', 'description', 'author_names', 'author_ids']

    # ------ Paper Table ------
    selected_cols = ['doi', 'citedby_count', 'description']

    paper_df = df[selected_cols].copy()

    # clean the data for nans
    paper_df = paper_df.dropna()

    paper_df.set_index('doi', inplace=True)

    # TODO: LORE, CHECK WHY THERE ARE NANS in SCOPUS!!!

    # NLP-process the data
    paper_df['digital_image_pickled'] = paper_df['description'].apply(lambda x: pickle.dumps(nlp_fn(x).vector))

    # Serialize the NumPy array
    # serialized_array = pickle.dumps(paper_df['digital_image'])

    # Create a DataFrame
    # paper_df = pd.DataFrame({'array': [serialized_array]})

    paper_df = paper_df.drop('description', axis=1)

    paper_df.to_sql('Paper', conn, if_exists='append', index=True, index_label='doi')

    # ------ Author Paper Table ------
    selected_cols = ['doi', 'citedby_count']  # selecting 2 columns to preserve DF instead of Series
    author_paper_df = df[selected_cols].copy()

    # clean the data for nans
    author_paper_df = author_paper_df.dropna()

    author_paper_df['author_id'] = author
    author_paper_df = author_paper_df.drop('citedby_count', axis=1)

    author_paper_df.to_sql('AuthorPaper', conn, if_exists='append', index=False)

    # ------ Author Table ------
    # TODO: later for Phase 2 of the project


def author_exists_db(author, conn):
    df_author_paper = pd.read_sql(f'SELECT * FROM AuthorPaper WHERE author_id = {author}', conn)

    n_papers = len(df_author_paper)
    if n_papers > 0:
        print(f'Found {n_papers} papers in DB for author {author}. Skipping SCOPUS call.')
        return True

    return False


def clean_up_db(conn):
    print('Cleaning up the DB')

    # ------ Paper Table ------
    clean_up_db_table(conn, 'Paper')

    # ------ Author Paper Table ------
    clean_up_db_table(conn, 'AuthorPaper')


def clean_up_db_table(conn, table_name):
    cur = conn.cursor()

    # Create a new table with the same structure
    cur.execute(f"CREATE TABLE {table_name}_new AS SELECT DISTINCT * FROM {table_name}")

    # Delete the old table
    cur.execute(f"DROP TABLE {table_name}")

    # Rename the new table to the old table's name
    cur.execute(f"ALTER TABLE {table_name}_new RENAME TO {table_name}")

    # Commit the changes and close the cursor
    conn.commit()
    cur.close()


# ------------------------------------------------------------

if __name__ == '__main__':
    api_key = "TODO"
    overwrite = False

    cache_scopus(overwrite)
