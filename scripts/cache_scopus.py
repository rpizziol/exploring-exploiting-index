import os
import sqlite3
import json
import pickle

import numpy as np
import pandas as pd
import spacy

import requests
from pybliometrics.scopus import ScopusSearch


def cache_scopus(overwrite=False):
    # read the authors list
    authors_path = os.path.join(os.path.dirname(__file__),
                                '..', 'data', 'txt', 'authors.txt')

    with open(authors_path, 'r') as file:
        authors = file.readlines()

    print(f'Found {len(authors)} authors.')

    # Prepare the database
    # Connect to SQLite database (will be created if not exists)
    db_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'db', 'e2index.db')
    conn = sqlite3.connect(db_path)

    # Prepare the NLP model
    nlp_fn = spacy.load('en_core_web_md')

    # for each author, get the papers from scopus
    for author in authors:
        # "clean" author
        # TODO make safer conversion
        author = int(author.strip())

        # TODO check that author is in the DB
        # if not overwrite:
        #     print(f'Author {author} already in the DB. Skipping.')
        #     continue

        # papers_df = get_papers_easy(author)

        # TODO REMOVE: interim write and read json to avoid API calls
        paper_csv_path = os.path.join(os.path.dirname(__file__),
                                      '..', 'data', 'csv', f'{author}.csv')
        # papers_df.to_csv(paper_csv_path, index=False)

        papers_df = pd.read_csv(paper_csv_path)

        # OBSOLETE
        # get the papers
        # print(f'Processing author: {author}')
        # papers_raw = get_papers_by_id(api_key, author)
        # if papers_raw is None:
        #     continue
        #
        # # TODO REMOVE: interim write and read json to avoid API calls
        # json_path_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'json', f'{author}.json')
        # with open(json_path_file, 'w') as file:
        #     json.dump(papers_raw, file)

        # with open(json_path_file, 'r') as file:
        #     papers_raw = json.load(file)

        # process the papers (from JSON to DataFrame)
        # papers_df = process_web_data(papers_raw)

        # save the papers in the database
        # print(f'Saving papers to the DB.')
        save_to_db(author, papers_df, conn, nlp_fn)

    # Bookkeeping
    conn.close()


def get_papers_by_id(api_key, author_id):
    papers_raw = None

    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "X-ELS-APIKey": api_key,
        "Accept": "application/json"
    }
    params = {
        "query": f"AU-ID({author_id})",
        "field": "doi,citedby-count"
    }

    response = requests.get(base_url, headers=headers, params=params)

    if response.status_code == 200:
        papers_raw = response.json()
    else:
        print("Failed to retrieve data:", response.status_code)

    return papers_raw


def process_web_data(papers_raw):
    # Navigate through JSON structure to find the papers data
    papers_list = papers_raw.get('search-results', {}).get('entry', [])

    # Extract relevant data for each paper
    processed_data = []
    for paper in papers_list:
        paper_data = {
            'DOI': paper.get('prism:doi', None),
            'citedby-count': paper.get('citedby-count', None)
        }
        processed_data.append(paper_data)

    # Create DataFrame
    papers_df = pd.DataFrame(processed_data)
    return papers_df


def get_papers_easy(author_id):
    print(f'Getting papers for author: {author_id}')

    # request to the SCOPUS API
    q = f"AU-ID({author_id})"  # any query that works in the Advanced Search on scopus.com
    s = ScopusSearch(q)

    # convert to DataFrame
    df = pd.DataFrame(s.results)

    # extract useful columns
    selected_cols = ['doi', 'citedby_count', 'description', 'author_names', 'author_ids']
    df = df[selected_cols]

    return df


def save_to_db(author, df, conn, nlp_fn):
    print(f'Saving papers to the DB for author: {author}')
    # Write 3 DataFrames to SQL tables
    # Author - author_id, author_name, *digital_image
    # Paper - doi, citedby_count, *digital_image
    # AuthorPaper - author_id, doi
    # prepare the data

    # Prepare
    # selected_cols = ['doi', 'citedby_count', 'description', 'author_names', 'author_ids']

    # ------ Paper Table ------
    selected_cols = ['doi', 'citedby_count', 'description']

    paper_df = df[selected_cols].copy()
    paper_df.set_index('doi', inplace=True)

    # clean the data for nans
    paper_df = paper_df.dropna()
    # TODO: LORE, CHECK WHY THERE ARE NANS !!!

    # NLP-process the data
    paper_df['digital_image_pickled'] = paper_df['description'].apply(lambda x: pickle.dumps(nlp_fn(x).vector))


    # Serialize the NumPy array
    # serialized_array = pickle.dumps(paper_df['digital_image'])

    # Create a DataFrame
    # paper_df = pd.DataFrame({'array': [serialized_array]})



    paper_df = paper_df.drop('description', axis=1)



    paper_df.to_sql('Paper', conn, if_exists='replace', index=True, index_label='doi')

    # Assuming 'df' is your DataFrame and 'citedby-count' is an existing column
    # df['new_column'] = df['citedby-count'].apply(lambda x: x * 2)  # Multiply each value by 2

    # Assuming 'df' is your DataFrame and 'citedby-count' is an existing column
    # df['new_column'] = df['citedby-count'].apply(lambda x: x * 2)  # Multiply each value by 2

    #
    # df_paper.set_index('Paper_ID', inplace=True)
    # df_paper.to_sql('Paper', conn, if_exists='replace', index=True, index_label='Paper_ID')
    #

    # # Write DataFrames to SQL tables
    # df_author.set_index('Author_ID', inplace=True)
    # df_author.to_sql('Author', conn, if_exists='replace', index=True, index_label='Author_ID')
    #
    # df_author_paper.to_sql('AuthorPaper', conn, if_exists='replace', index=False)


# ------------------------------------------------------------

if __name__ == '__main__':
    api_key = "TODO"
    overwrite = False

    cache_scopus(overwrite)
